[pipeline]
# The model that will be used by the pipeline to make predictions.
# The model must be an instance of a pretrained model inheriting from PreTrainedModel or TFPreTrainedModel.
model = microsoft/DialoGPT-medium

# The configuration that will be used by the pipeline to instantiate the model.
config

# The tokenizer that will be used by the pipeline to encode data for the model.
tokenizer

# The framework to use, either pt for PyTorch or tf for TensorFlow.
# The specified framework must be installed.
framework

# Device ordinal for CPU/GPU supports.
# Setting this to -1 will leverage CPU, a positive will run the model on the associated CUDA device id.
device = -1

[generator]
# See https://huggingface.co/blog/how-to-generate

# Seed for random number generators, fix seed to reproduce results.
# By default there is no seed and each turn should be unique.
seed

# The maximal number of tokens to be returned, inclusive of punctuations etc.
# It will automatically stop if the end-of-sequence token was found earlier.
max_length = 128

# The minimum length of the sequence to be generated.
min_length = 1

# Whether or not to use sampling; use greedy decoding otherwise.
do_sample = True

# Whether to stop the beam search when at least num_beams sentences are finished per batch or not.
early_stopping = False

# Number of beams for beam search. 1 means no beam search.
num_beams = 1

# The value used to module the next token probabilities.
# Lower temperature results in less random completions.
# As the temperature approaches zero, the model will become deterministic and repetitive.
# Higher temperature results in more random completions.
temperature = 0.7

# The number of highest probability vocabulary tokens to keep for top-k-filtering.
# 1 means only 1 word is considered for each step (token), resulting in deterministic completions,
# while 40 means 40 words are considered at each step.
# 0 (default) is a special setting meaning no restrictions.
# 40 generally is a good value.
top_k = 40

# If set to float < 1, only the most probable tokens with probabilities
# that add up to top_p or higher are kept for generation.
top_p = 1.0

# The parameter for repetition penalty. 1.0 means no penalty.
repetition_penalty = 1.0

# The id of the padding token.
pad_token_id

# The id of the beginning-of-sequence token.
bos_token_id

# The id of the end-of-sequence token.
eos_token_id

# Exponential penalty to the length. 1.0 means no penalty.
# Set to values < 1.0 in order to encourage the model to generate shorter sequences,
# to a value > 1.0 in order to encourage the model to produce longer sequences.
length_penalty = 1.0

# If set to int > 0, all ngrams of that size can only occur once.
no_repeat_ngram_size = 0

# Comma separated list of token ids that are not allowed to be generated.
bad_words_ids

# The number of independently computed returned sequences for each element in the batch.
# You would have to implement a function (message_selector) to choose one message from generated list.
# For example, you can choose the most dissimilar message, or the lengthiest one.
# But keep in mind: the higher the number, the slower the generation.
num_return_sequences = 1

# If an encoder-decoder model starts decoding with a different token than bos, the id of that token.
decoder_start_token_id

# Whether or not the model should use the past last key/values attentions
# (if applicable to the model) to speed up decoding.
use_cache = True

# Whether or not to clean up the potential extra spaces in the text output.
clean_up_tokenization_spaces = True

[chatbot]

# The number of turns (turn = answer and response) the model should consider.
# Set to 0 to focus on the last message. Set to -1 for unlimited context length.
max_turns_history = 2

# Your Telegram token. See https://core.telegram.org/bots.
telegram_token = 1297407265:AAEELY2DL-EQxIP-NC7GfutvVTDvDwl23qQ

# Your GIPHY API token. See 
giphy_token = jZqQgNs4ZLYXmyIqApNWP0p4fXsC8kFh

# Probability of returning a GIF.
giphy_prob = 0.1

# The maximal number of words the bot has to generate to also return a GIF.
giphy_max_words = 3

# Value from 0-10 which makes results weirder as you go up the scale.
giphy_weirdness = 5
